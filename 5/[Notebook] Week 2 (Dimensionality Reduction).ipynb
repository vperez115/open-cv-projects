{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRj-iblEQGi8"
   },
   "source": [
    "# Dimensionality Reduction\n",
    "Computer vision often deals with high dimensional data. As the dimensions of data increase so do its features (More dimensions also act as noise rather than features).\n",
    "Dimensionality reduction is important in CV as it reduces the number of input variables,  making the CV algorithms faster and more efficient.\n",
    "In this notebook we will explore dimensionality reduction with Principal Component Analysis and T- Distributed Stochastic Neighbourhood embedding (T-SNE).\n",
    "\n",
    "Let's begin by loading the MNIST  (Modified National Institute of Standards and Technology) Dataset- a large database of handwritten digits that is commonly used for training various image processing systems.\n",
    "\n",
    "Download the MNIST Dataset from https://www.kaggle.com/c/digit-recognizer/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "id": "wp-ij13HQGi-",
    "outputId": "4ee335ae-8b2d-434e-b216-58387aef5edc"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Functions to read and show images.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Functions to read and show images.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d0 = pd.read_csv('mnist_train.csv')\n",
    "\n",
    "print(d0.head(5)) # print first five rows of d0.\n",
    "\n",
    "# save the labels into a variable l.\n",
    "l = d0['label']\n",
    "\n",
    "# Drop the label feature and store the pixel data in d.\n",
    "d = d0.drop(\"label\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "TTAYdlDqQGjE",
    "outputId": "ce592731-6219-4fc0-bbd5-f98e526eb275"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43md\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(l\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "print(d.shape)\n",
    "print(l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "OFPxxtvlQGjJ",
    "outputId": "aa2580b6-39c3-4722-87c5-9b59d035a67c"
   },
   "outputs": [],
   "source": [
    "# display or plot a number.\n",
    "plt.figure(figsize=(7,7))\n",
    "idx = 1\n",
    "\n",
    "grid_data = d.iloc[idx].to_numpy().reshape(28,28)  # reshape from 1d to 2d pixel array\n",
    "plt.imshow(grid_data, interpolation = \"none\", cmap = \"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(l[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uF4ViytmQGjO"
   },
   "source": [
    "# Principal Component Analysis (PCA) for 2D visualization\n",
    " \n",
    "Principal Component Analysis(PCA) is a linear dimension reduction algorithm. It is a projection based method that transforms the data by projecting it onto a set of orthogonal(perpendicular) axes. \n",
    "\n",
    "Let us further explore PCA with the MNIST dataset. In this segment we are using PCA only to visualize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "6RWnz578QGjP",
    "outputId": "0b4b5b1b-96a5-492c-aa79-b95422bfb4e3"
   },
   "outputs": [],
   "source": [
    "# Pick first 15K data-points to work on for time-effeciency.\n",
    "# Excercise: Perform the same analysis on all of 42K data-points.\n",
    "\n",
    "labels = l.head(15000)\n",
    "data = d.head(15000)\n",
    "\n",
    "print(\"the shape of sample data = \", data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "9qWXuMacQGjU",
    "outputId": "d2f6d210-cd15-4e56-9d7f-8f05fb6af6ac"
   },
   "outputs": [],
   "source": [
    "# Data-preprocessing: Standardizing the data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standardized_data = StandardScaler().fit_transform(data)\n",
    "print(standardized_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ho9hboDkQGjb",
    "outputId": "68a75918-5984-4d42-9065-51c2eff0651c"
   },
   "outputs": [],
   "source": [
    "#find the co-variance matrix which is : A^T * A\n",
    "sample_data = standardized_data\n",
    "\n",
    "# matrix multiplication using numpy\n",
    "covar_matrix = np.matmul(sample_data.T , sample_data)\n",
    "\n",
    "print ( \"The shape of variance matrix = \", covar_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "mtlKyc-9QGjg",
    "outputId": "235983d4-39dc-40e8-8d19-44d2179554e2"
   },
   "outputs": [],
   "source": [
    "# finding the top two eigen-values and corresponding eigen-vectors. \n",
    "# for projecting onto a 2-Dim space.\n",
    "\n",
    "from scipy.linalg import eigh \n",
    "\n",
    "# the parameter 'eigvals' is defined (low value to heigh value). \n",
    "# eigh function will return the eigen values in asending order.\n",
    "# this code generates only the top 2 (782 and 783) eigenvalues.\n",
    "values, vectors = eigh(covar_matrix, eigvals=(782,783))\n",
    "\n",
    "print(\"Shape of eigen vectors = \",vectors.shape)\n",
    "# converting the eigen vectors into (2,d) shape for easyness of further computations.\n",
    "vectors = vectors.T\n",
    "\n",
    "print(\"Updated shape of eigen vectors = \",vectors.shape)\n",
    "# here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector.\n",
    "# here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "zfQYzUaKQGjl",
    "outputId": "1071d016-2a1c-4511-9579-a526f10b04ea"
   },
   "outputs": [],
   "source": [
    "# projecting the original data sample on the plane. \n",
    "# formed by two principal eigen vectors by vector-vector multiplication.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "new_coordinates = np.matmul(vectors, sample_data.T)\n",
    "\n",
    "print (\" resultanat new data points' shape \", vectors.shape, \"X\", sample_data.T.shape,\" = \", new_coordinates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "q_5hc304QGjp",
    "outputId": "ab87217d-da90-4d08-f779-29f2adbbd81e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# appending label to the 2d projected data.\n",
    "new_coordinates = np.vstack((new_coordinates, labels)).T\n",
    "\n",
    "# creating a new data frame for ploting the labeled points.\n",
    "dataframe = pd.DataFrame(data=new_coordinates, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cz7q6UCbvA4K"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame()\n",
    "df['1st']=[-5.558661,-5.043558,6.193635 ,19.305278]\n",
    "df['2nd']=[-1.558661,-2.043558,2.193635 ,9.305278]\n",
    "df['label']=[1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "tphijoAQu4xi",
    "outputId": "d89f0efc-e2b2-4314-ff68-fb2ba08c7fdc"
   },
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "sn.FacetGrid(df, hue=\"label\", height=6).map(plt.scatter, '1st', '2nd').add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "0XGcis_bvzNG",
    "outputId": "caa2d2b0-b7d3-40e2-9b20-8078cf064e56"
   },
   "outputs": [],
   "source": [
    "sn.scatterplot(x=\"1st\",y=\"2nd\",hue=\"label\",data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "fGQ6V_LiQGjs",
    "outputId": "29fbef95-e85f-4ef3-c542-3b7426f3580a"
   },
   "outputs": [],
   "source": [
    "# ploting the 2d data points with seaborn\n",
    "import seaborn as sn\n",
    "sn.FacetGrid(dataframe, hue=\"label\", height=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "drHOQV6w0Kao",
    "outputId": "22ce5735-36ba-43a8-dea8-88b3c4e1bac6"
   },
   "outputs": [],
   "source": [
    "sn.scatterplot(x=\"1st_principal\",y=\"2nd_principal\",legend=\"full\",hue=\"label\",data=dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g88y_UkgQGjw"
   },
   "source": [
    "# PCA using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Od_a_6uQGjx"
   },
   "outputs": [],
   "source": [
    "# initializing the pca\n",
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_Y6hEhEQGjy",
    "outputId": "94143f7b-fc55-4ffc-c8a0-5e6e44a488a2"
   },
   "outputs": [],
   "source": [
    "# configuring the parameteres.\n",
    "# the number of components = 2\n",
    "pca.n_components = 2\n",
    "pca_data = pca.fit_transform(sample_data)\n",
    "\n",
    "# pca_reduced will contain the 2-d projects of simple data.\n",
    "print(\"shape of pca_reduced.shape = \", pca_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tISf8ZwJQGj1",
    "outputId": "dbf43b79-b3ec-435b-8109-9df804773a40"
   },
   "outputs": [],
   "source": [
    "# attaching the label for each 2-d data point \n",
    "pca_data = np.vstack((pca_data.T, labels)).T\n",
    "\n",
    "# creating a new data fram which help us in ploting the result data\n",
    "pca_df = pd.DataFrame(data=pca_data, columns=(\"1st_principal\", \"2nd_principal\", \"label\"))\n",
    "sn.FacetGrid(pca_df, hue=\"label\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfyEnV8-QGj4"
   },
   "source": [
    "# PCA for dimensionality redcution (not for visualization)\n",
    "PCA creates new variables by combining the original variables in such a way that the variance is maximized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8blOjJYQGj5",
    "outputId": "d94f2779-d78e-45dc-a99f-28b6d34340fc"
   },
   "outputs": [],
   "source": [
    "# PCA for dimensionality redcution (non-visualization)\n",
    "\n",
    "pca.n_components = 784\n",
    "pca_data = pca.fit_transform(sample_data)\n",
    "\n",
    "percentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_);\n",
    "\n",
    "cum_var_explained = np.cumsum(percentage_var_explained)\n",
    "\n",
    "# Plot the PCA spectrum\n",
    "plt.figure(1, figsize=(6, 4))\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(cum_var_explained, linewidth=2)\n",
    "plt.axis('tight')\n",
    "plt.grid()\n",
    "plt.xlabel('n_components')\n",
    "plt.ylabel('Cumulative_explained_variance')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# If we take 200-dimensions, approx. 90% of variance is expalined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTaRTugBQGj7"
   },
   "source": [
    "# t-SNE using Scikit-Learn\n",
    "t-SNE is a nonlinear dimensionality reduction technique that is well suited for embedding high dimension data into lower dimensional data (2D or 3D) for data visualization. While PCA works on retaining only global variance, t-SNE works on retaining local variance- a shortfall of PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd2bHAybQGj8",
    "outputId": "5a33ffa1-6459-4fbc-f935-1c341b6dac3c"
   },
   "outputs": [],
   "source": [
    "# TSNE\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Picking the top 1000 points as TSNE takes a lot of time for 15K points\n",
    "data_1000 = standardized_data[0:1000,:]\n",
    "labels_1000 = labels[0:1000]\n",
    "\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "# configuring the parameteres\n",
    "# the number of components = 2\n",
    "# default perplexity = 30\n",
    "# default learning rate = 200\n",
    "# default Maximum number of iterations for the optimization = 1000\n",
    "\n",
    "tsne_data = model.fit_transform(data_1000)\n",
    "\n",
    "\n",
    "# creating a new data frame which help us in ploting the result data\n",
    "tsne_data = np.vstack((tsne_data.T, labels_1000)).T\n",
    "tsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n",
    "\n",
    "# Ploting the result of tsne\n",
    "sn.FacetGrid(tsne_df, hue=\"label\", height=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-jgsYj8QGj_",
    "outputId": "eb0d26ab-1644-403c-8ebf-79e3514f438d"
   },
   "outputs": [],
   "source": [
    "model = TSNE(n_components=2, random_state=0, perplexity=50)\n",
    "tsne_data = model.fit_transform(data_1000) \n",
    "\n",
    "# creating a new data fram which help us in ploting the result data\n",
    "tsne_data = np.vstack((tsne_data.T, labels_1000)).T\n",
    "tsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n",
    "\n",
    "# Ploting the result of tsne\n",
    "sn.FacetGrid(tsne_df, hue=\"label\", height=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\n",
    "plt.title('With perplexity = 50')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCQ460syQGkC",
    "outputId": "3b29a34c-8f2e-4afa-9eca-a27036b8ec90"
   },
   "outputs": [],
   "source": [
    "model = TSNE(n_components=2, random_state=0, perplexity=50,  n_iter=5000)\n",
    "tsne_data = model.fit_transform(data_1000) \n",
    "\n",
    "# creating a new data fram which help us in ploting the result data\n",
    "tsne_data = np.vstack((tsne_data.T, labels_1000)).T\n",
    "tsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n",
    "\n",
    "# Ploting the result of tsne\n",
    "sn.FacetGrid(tsne_df, hue=\"label\", height=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\n",
    "plt.title('With perplexity = 50, n_iter=5000')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkcpBB1MQGkF",
    "outputId": "7634e2ae-a60f-470c-d453-a850a5d7e1b0"
   },
   "outputs": [],
   "source": [
    "model = TSNE(n_components=2, random_state=0, perplexity=2)\n",
    "tsne_data = model.fit_transform(data_1000) \n",
    "\n",
    "# creating a new data fram which help us in ploting the result data\n",
    "tsne_data = np.vstack((tsne_data.T, labels_1000)).T\n",
    "tsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n",
    "\n",
    "# Ploting the result of tsne\n",
    "sn.FacetGrid(tsne_df, hue=\"label\", height=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\n",
    "plt.title('With perplexity = 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvMINUG_QGkH"
   },
   "outputs": [],
   "source": [
    "Run the same analysis using 42K points with various \n",
    "#values of perplexity and iterations.\n",
    "\n",
    "# If you use all of the points, you can expect plots like this blog below:\n",
    "# http://colah.github.io/posts/2014-10-Visualizing-MNIST/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6T6jHFXQGkK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "13.10#14.9#14.10#15.7.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
